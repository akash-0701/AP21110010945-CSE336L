{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8d0e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE: 5.624242424242423\n",
      "R^2: 0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "mean_x = np.mean(x)\n",
    "mean_y = np.mean(y)\n",
    "\n",
    "numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "denominator = np.sum((x - mean_x) ** 2)\n",
    "slope = numerator / denominator\n",
    "intercept = mean_y - slope * mean_x\n",
    "\n",
    "y_pred = slope * x + intercept\n",
    "\n",
    "SSE = np.sum((y - y_pred) ** 2)\n",
    "\n",
    "SST = np.sum((y - mean_y) ** 2)\n",
    "R_squared = 1 - (SSE / SST)\n",
    "\n",
    "\n",
    "print(\"SSE:\", SSE)\n",
    "print(\"R^2:\", R_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a02c693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-batch Gradient Descent:\n",
      "\n",
      "Coefficients: [5.54566298 1.32488576]\n",
      "SE: 463.0733655195668\n",
      "R-squared value: 0.22355237169757425\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "\n",
      "Coefficients: [2.7364106 2.7364106]\n",
      "SSE: 758.5518927149384\n",
      "R-squared value: -0.271884461292653\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 25, 17, 8, 18, 19, 11, 12])\n",
    "\n",
    "\n",
    "def full_batch_gradient_descent(x, y, learning_rate, num_iterations):\n",
    "\n",
    "    theta = np.zeros(2)\n",
    "    m = len(y)\n",
    "    X = np.vstack((np.ones_like(x), x)).T\n",
    "    for _ in range(num_iterations):\n",
    "        y_pred = X.dot(theta)\n",
    "        theta -= (1/m) * learning_rate * X.T.dot(y_pred - y)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "def stochastic_gradient_descent(x, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(2)\n",
    "    m = len(y)\n",
    "    for _ in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            rand_index = np.random.randint(0, m)\n",
    "            xi = x[rand_index]\n",
    "            yi = y[rand_index]\n",
    "            y_pred = np.dot(xi, theta)\n",
    "            theta -= learning_rate * xi * (y_pred - yi)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "theta_full_batch = full_batch_gradient_descent(x, y, learning_rate, num_iterations)\n",
    "theta_stochastic = stochastic_gradient_descent(x, y, learning_rate, num_iterations)\n",
    "\n",
    "X = np.vstack((np.ones_like(x), x)).T\n",
    "\n",
    "y_pred_full_batch = X.dot(theta_full_batch)\n",
    "y_pred_stochastic = X.dot(theta_stochastic)\n",
    "\n",
    "SSE_full_batch = np.sum((y - y_pred_full_batch) ** 2)\n",
    "SSE_stochastic = np.sum((y - y_pred_stochastic) ** 2)\n",
    "\n",
    "mean_y = np.mean(y)\n",
    "SST = np.sum((y - mean_y) ** 2)\n",
    "R_squared_full_batch = 1 - (SSE_full_batch / SST)\n",
    "R_squared_stochastic = 1 - (SSE_stochastic / SST)\n",
    "\n",
    "# Step 8: Print results\n",
    "print(\"Full-batch Gradient Descent:\")\n",
    "print(\"\\nCoefficients:\", theta_full_batch)\n",
    "print(\"SE:\", SSE_full_batch)\n",
    "print(\"R-squared value:\", R_squared_full_batch)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"\\nCoefficients:\", theta_stochastic)\n",
    "print(\"SSE:\", SSE_stochastic)\n",
    "print(\"R-squared value:\", R_squared_stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd4c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients using Analytic Formulation: [44459.72916908 41933.84939381]\n",
      "Coefficients using Full-batch Gradient Descent: [39148.47787113 43047.96802282]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing_data = pd.read_csv(\"housing.csv\")\n",
    "\n",
    "selected_attribute = 'median_income'\n",
    "X = housing_data[selected_attribute].values.reshape(-1, 1)\n",
    "y = housing_data['median_house_value'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_with_intercept = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_with_intercept = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "theta_analytic = np.linalg.inv(X_train_with_intercept.T.dot(X_train_with_intercept)).dot(X_train_with_intercept.T).dot(y_train)\n",
    "print(\"Coefficients using Analytic Formulation:\", theta_analytic)\n",
    "\n",
    "def full_batch_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(num_iterations):\n",
    "        y_pred = X.dot(theta)\n",
    "        theta -= (1/len(y)) * learning_rate * X.T.dot(y_pred - y)\n",
    "    return theta\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "theta_full_batch = full_batch_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Full-batch Gradient Descent:\", theta_full_batch)\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(num_iterations):\n",
    "        for i in range(len(y)):\n",
    "            rand_index = np.random.randint(0, len(y))\n",
    "            xi = X[rand_index]\n",
    "            yi = y[rand_index]\n",
    "            y_pred = np.dot(xi, theta)\n",
    "            theta -= learning_rate * xi * (y_pred - yi)\n",
    "    return theta\n",
    "\n",
    "theta_stochastic = stochastic_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Stochastic Gradient Descent:\", theta_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c51cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
